vocab:
    vocab_size: 5000
    pad_id: 0
    unk_id: 1
    bos_id: 2
    eos_id: 3
    pad_token: "[PAD]"
    unk_token: "[UNK]"
    bos_token: "[BOS]"
    eos_token: "[EOS]"


model:
    emb_dim: 256
    hidden_dim: 512
    n_layers: 2
    dropout_ratio: 0.5


train:
    clip: 1
    early_stop: 1
    patience: 3
    n_epochs: 10
    batch_size: 64
    learning_rate: 0.001
    iters_to_accumulate: 4